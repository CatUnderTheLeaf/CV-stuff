{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f09adc8",
   "metadata": {},
   "source": [
    "# Download and convert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of JSON file URLs\n",
    "urls = [\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_train2020.json\",\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_val2020.json\",\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/annotations/info_test2020.json\",\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/annotations/attributes_train2020.json\",\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/annotations/attributes_val2020.json\"\n",
    "]\n",
    "\n",
    "# Download each file into current directory (/content)\n",
    "for url in urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    !wget -O $filename $url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = [\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip\",\n",
    "    \"https://s3.amazonaws.com/ifashionist-dataset/images/val_test2020.zip\"\n",
    "]\n",
    "\n",
    "for url in zip_urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    !wget -q --show-progress -O $filename $url\n",
    "    \n",
    "    print(f\"Extracting {filename}...\")\n",
    "    !unzip -o $filename > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd447cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f94fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/\"\n",
    "img_train_root = dataset_path + \"train/\"\n",
    "img_val_root = dataset_path + \"test/\"\n",
    "\n",
    "# original names\n",
    "ann_train_file = \"instances_attributes_train2020.json\"\n",
    "ann_val_file = \"instances_attributes_val2020.json\"\n",
    "# ann_train_file = \"attributes_train2020.json\"\n",
    "# ann_val_file = \"attributes_val2020.json\"\n",
    "\n",
    "info_file = \"info_test2020.json\"\n",
    "\n",
    "# for commercial use names\n",
    "ann_com_train = \"instances_attributes_commercial_train_filtered.csv\"\n",
    "ann_com_val = \"instances_attributes_commercial_val_filtered.csv\"\n",
    "# ann_com_train = \"attributes_commercial_train.csv\"\n",
    "# ann_com_val = \"attributes_commercial_val.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af46a80",
   "metadata": {},
   "source": [
    "### Remove images with licenses unsuitable for commercial use and remove annotations with not needed categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812913fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original images: 1158\n",
      "Filtered images: 330\n",
      "   image_id  category_id                                     attribute_ids  \\\n",
      "0     17039           31                                        [160, 204]   \n",
      "1     17039           31                                        [160, 204]   \n",
      "2     17039            9  [229, 295, 136, 137, 80, 145, 115, 85, 311, 317]   \n",
      "3     17039           25                                                []   \n",
      "4     17039           27                                                []   \n",
      "\n",
      "                           bbox  width  height  \\\n",
      "0   [391.0, 460.0, 75.0, 193.0]   1024     682   \n",
      "1   [583.0, 452.0, 50.0, 228.0]   1024     682   \n",
      "2  [392.0, 407.0, 241.0, 274.0]   1024     682   \n",
      "3  [460.0, 389.0, 136.0, 109.0]   1024     682   \n",
      "4   [442.0, 407.0, 166.0, 51.0]   1024     682   \n",
      "\n",
      "                              file_name  license  \n",
      "0  99601fa457d157b81154d089966c2e3a.jpg        7  \n",
      "1  99601fa457d157b81154d089966c2e3a.jpg        7  \n",
      "2  99601fa457d157b81154d089966c2e3a.jpg        7  \n",
      "3  99601fa457d157b81154d089966c2e3a.jpg        7  \n",
      "4  99601fa457d157b81154d089966c2e3a.jpg        7  \n"
     ]
    }
   ],
   "source": [
    "# ['glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'leg warmer', \n",
    "# 'tights, stockings', 'sock', 'umbrella', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon',\n",
    "#  'rivet', 'ruffle', 'sequin', 'tassel']\n",
    "\n",
    "def filter_categories(df):\n",
    "    remove_cat_ids = [13, 14, 15, 16, 17, 18, 20, 21, 22, 26, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
    "    return df[~df[\"category_id\"].isin(remove_cat_ids)].copy()\n",
    "\n",
    "# Load JSON\n",
    "def load_cat_attr(dataset_path, filename):\n",
    "    with open(dataset_path+filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df_attributes = pd.DataFrame(data[\"attributes\"])\n",
    "    df_categories = pd.DataFrame(data[\"categories\"])\n",
    "    return df_categories, df_attributes\n",
    "\n",
    "df_categories, df_attributes = load_cat_attr(dataset_path, info_file)\n",
    "\n",
    "id_to_attr_name = df_attributes.set_index('id')['name'].to_dict()\n",
    "id_to_cat_name = df_categories.set_index('id')['name'].to_dict()\n",
    "\n",
    "# Count the frequency of each category\n",
    "def plot_counts(df):\n",
    "    category_counts = df[\"category_id\"].value_counts()\n",
    "\n",
    "    # Map IDs â†’ names\n",
    "    category_counts.index = category_counts.index.map(id_to_cat_name)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    category_counts.plot(kind=\"bar\")\n",
    "\n",
    "    plt.title(\"Distribution of Categories\")\n",
    "    plt.xlabel(\"Category ID\")\n",
    "    plt.ylabel(\"Number of Annotations\")\n",
    "    plt.show()\n",
    "\n",
    "def convert_json_to_csv(dataset_path, filename, new_filename):\n",
    "    with open(dataset_path+filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df_annotations = pd.DataFrame(data[\"annotations\"])\n",
    "    df_images = pd.DataFrame(data[\"images\"])\n",
    "\n",
    "    # safe for commercial use: 0, 1, 6, 7, 8, 9, 10\n",
    "    allowed_ids = [0, 1, 6, 7, 8, 9, 10]\n",
    "    # keep images only with allowed licences\n",
    "    df_images_filtered = df_images[df_images['license'].isin(allowed_ids)].copy()\n",
    "    # print(df_images_filtered.head())\n",
    "\n",
    "    print(f\"Original images: {len(df_images)}\")\n",
    "    print(f\"Filtered images: {len(df_images_filtered)}\")\n",
    "\n",
    "    # print(df_annotations.head())\n",
    "    # merge images with attributes and drop not useful columns\n",
    "    df_annotations_filtered = df_annotations.merge(\n",
    "        df_images_filtered,\n",
    "        left_on='image_id',\n",
    "        right_on='id',\n",
    "        how='inner'\n",
    "    ).drop(columns=['id_x', 'id_y', 'time_captured', 'segmentation', 'area', 'iscrowd', 'original_url', 'isstatic', 'kaggle_id'])\n",
    "    # print(df_annotations_filtered.head())\n",
    "    # save to a csv file\n",
    "\n",
    "    df_annotations_filtered = filter_categories(df_annotations_filtered)\n",
    "    plot_counts(df_annotations_filtered)\n",
    "    df_annotations_filtered.to_csv(dataset_path+new_filename,index=False)\n",
    "\n",
    "\n",
    "# uncomment to convert and filter dataset\n",
    "convert_json_to_csv(dataset_path, ann_train_file, ann_com_train)\n",
    "convert_json_to_csv(dataset_path, ann_val_file, ann_com_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf10db",
   "metadata": {},
   "source": [
    "# Load data with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from super_gradients.training.utils.collate_fn.detection_collate_fn import DetectionCollateFN\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be927f1",
   "metadata": {},
   "source": [
    "#### Create custom Dataset class that loads data from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_image(image, max_size=640, color=(114, 114, 114)):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = max_size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    resized_image = cv2.resize(image, (new_w, new_h))\n",
    "    \n",
    "    # Create padded image\n",
    "    padded_image = np.full((max_size, max_size, 3), color, dtype=np.uint8)\n",
    "    \n",
    "    # Compute top-left corner for placing resized image\n",
    "    top = (max_size - new_h) // 2\n",
    "    left = (max_size - new_w) // 2\n",
    "    \n",
    "    padded_image[top:top+new_h, left:left+new_w] = resized_image\n",
    "    return padded_image, scale, left, top\n",
    "\n",
    "def coco_to_yolo_all(boxes, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert multiple COCO bboxes to YOLO format.\n",
    "\n",
    "    Parameters:\n",
    "        boxes: numpy array of shape (N, 4), COCO format [x_min, y_min, w, h]\n",
    "        img_width: int\n",
    "        img_height: int\n",
    "\n",
    "    Returns:\n",
    "        yolo_boxes: numpy array of shape (N, 4), YOLO format [x_center, y_center, w, h]\n",
    "    \"\"\"\n",
    "    # Compute center coordinates\n",
    "    x_center = boxes[:, 0] + boxes[:, 2] / 2\n",
    "    y_center = boxes[:, 1] + boxes[:, 3] / 2\n",
    "\n",
    "    # Normalize\n",
    "    x_center_norm = x_center / img_width\n",
    "    y_center_norm = y_center / img_height\n",
    "    w_norm = boxes[:, 2] / img_width\n",
    "    h_norm = boxes[:, 3] / img_height\n",
    "\n",
    "    yolo_boxes = np.stack([x_center_norm, y_center_norm, w_norm, h_norm], axis=1)\n",
    "    return yolo_boxes\n",
    "\n",
    "\n",
    "def convert_image_to_yolo(image, allboxes, max_size=640):\n",
    "    \"\"\"\n",
    "    Convert an image and its bounding boxes to a format suitable for YOLO training.\n",
    "\n",
    "    Parameters:\n",
    "        image: numpy array of shape (H, W, C)\n",
    "        boxes: numpy array of shape (N, 4) with COCO format [x_min, y_min, width, height]\n",
    "        max_size: int, size to which the image will be resized \n",
    "\n",
    "    Returns:\n",
    "        image_tensor: torch.Tensor of shape (C, H, W)\n",
    "        target_array: numpy array of shape (N, 5) with YOLO format [class_label, x_center_norm, y_center_norm, width_norm, height_norm]\n",
    "    \"\"\"\n",
    "\n",
    "    # Letterbox resize\n",
    "    padded_img, scale, pad_left, pad_top = letterbox_image(image, max_size)\n",
    "\n",
    "    # Adjust bounding boxes after resizing + padding\n",
    "    boxes = allboxes * scale\n",
    "    boxes[:, 0] += pad_left   # x\n",
    "    boxes[:, 1] += pad_top    # y\n",
    "    new_boxes = coco_to_yolo_all(boxes, max_size, max_size)\n",
    "\n",
    "    return padded_img, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, max_size=640):\n",
    "        \"\"\"\n",
    "        :param csv_file: path to the annotations csv file\n",
    "        :param images_dir: folder where images are stored\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Group annotations by image for easier retrieval\n",
    "        self.image_groups = self.annotations.groupby(\"image_id\")\n",
    "\n",
    "        # Keep unique image rows\n",
    "        self.image_infos = self.annotations.drop_duplicates(\"image_id\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get one image info row\n",
    "        img_info = self.image_infos.iloc[idx]\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        # Load image with OpenCV (BGR)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get annotations for this image\n",
    "        annots = self.image_groups.get_group(img_info[\"image_id\"])\n",
    "        boxes = np.stack(annots[\"bbox\"].apply(eval).values)  # [x-top-left,y-top-left,w,h]\n",
    "        labels = annots[\"category_id\"].values\n",
    "        # attributes = annots[\"attribute_ids\"].apply(lambda x: eval(x) if isinstance(x, str) else []).values\n",
    "        \n",
    "        # image should be resized to max_size x max_size\n",
    "        # and padded with some color if necessary\n",
    "        # yolo takes bbox in [normalized_x_center, normalized_y_center, normalized_bbox_width, normalized_bbox_height] format\n",
    "        padded_img, padded_boxes = convert_image_to_yolo(image, boxes, self.max_size)\n",
    "\n",
    "        image_tensor = torch.from_numpy(padded_img).permute(2, 0, 1).float()\n",
    "        # target = {\n",
    "        #     \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "        #     \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            # \"attributes\": attributes,\n",
    "            # \"image_id\": torch.tensor(img_info[\"image_id\"]),\n",
    "        # }\n",
    "        target_array = np.hstack((labels.reshape(-1, 1), padded_boxes))\n",
    "\n",
    "        return image_tensor, target_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15201222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomCSVDataset(dataset_path+ann_com_train, img_train_root)\n",
    "val_dataset = CustomCSVDataset(dataset_path+ann_com_val, img_val_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e2da1",
   "metadata": {},
   "source": [
    "##### Examples of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ac00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f241671",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b47f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[1][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd316c",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=DetectionCollateFN())\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=DetectionCollateFN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b274ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e83da",
   "metadata": {},
   "source": [
    "# Model and its params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import super_gradients\n",
    "from super_gradients.training import models\n",
    "from super_gradients import Trainer\n",
    "from super_gradients.training import training_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get(\"yolox_n\", num_classes=NUM_CLASSES, pretrained_weights=\"coco\")\n",
    "print('Num classes in the model:', model.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a951bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = training_hyperparams.get('coco2017_yolox')\n",
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37620a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit params\n",
    "train_params['max_epochs'] = 10\n",
    "train_params['lr_warmup_epochs'] = 0\n",
    "train_params['lr_cooldown_epochs'] = 0\n",
    "train_params['criterion_params']['num_classes'] = NUM_CLASSES\n",
    "train_params['average_best_models'] = False\n",
    "train_params['initial_lr'] = 0.0005\n",
    "train_params['cosine_final_lr_ratio'] = 0.9\n",
    "train_params['mixed_precision'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141b5e8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70af38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = '/ckpt_folder' # Local path\n",
    "trainer = Trainer(experiment_name='transfer_learning_object_detection_yolox', ckpt_root_dir=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model=model, training_params=train_params, train_loader=train_dataloader, valid_loader=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3204b9c",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce95a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://github.com/KMnP/fashionpedia-api/blob/1ef732050e15d446c38d58ef945ccadc28c59328/images/000000010223.jpg\"\n",
    "# url = \"https://github.com/KMnP/fashionpedia-api/blob/1ef732050e15d446c38d58ef945ccadc28c59328/images/000000009813.jpg\"\n",
    "# url = \"https://github.com/KMnP/fashionpedia-api/blob/1ef732050e15d446c38d58ef945ccadc28c59328/data/demo/input.jpg\"\n",
    "# prediction = model.predict(url)\n",
    "# prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results:\n",
    "# \"https://github.com/KMnP/fashionpedia-api/blob/1ef732050e15d446c38d58ef945ccadc28c59328/data/demo/result.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a7a92",
   "metadata": {},
   "source": [
    "# Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f588499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import locale\n",
    "# locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx_graphsurgeon==0.3.27 --extra-index-url https://pypi.ngc.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81549778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_result = model.export(output=\"myexport.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
